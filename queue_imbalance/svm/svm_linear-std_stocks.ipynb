{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM with linear kernel\n",
    "\n",
    "The goal of this notebook is to find the best parameters for linear kernel. We also want to check if the parameters depend on stock.\n",
    "\n",
    "Linear kernel is a function: $\\langle x, x'\\rangle$.\n",
    "\n",
    "We will use [sklearn.svm](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) library to perform calculations. We want to pick the best parameters for **SVC**:\n",
    "\n",
    "* C (default 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as md\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "import warnings\n",
    "\n",
    "from lob_data_utils import lob\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We use data from 5 stocks (from dates 2013-09-01 - 2013-11-16) for which logistic regression yielded the best results.\n",
    "\n",
    "We selected 3 subsets for each stock:\n",
    "* training set (60% of data)\n",
    "* test set (20% of data)\n",
    "* cross-validation set (20% of data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length for 9061: 10347\n",
      "Testing set length for 9061: 3449\n",
      "Cross-validation set length for 9061: 3449\n",
      "Training set length for 9062: 10531\n",
      "Testing set length for 9062: 3510\n",
      "Cross-validation set length for 9062: 3510\n",
      "Training set length for 9063: 9921\n",
      "Testing set length for 9063: 3306\n",
      "Cross-validation set length for 9063: 3306\n",
      "Training set length for 9064: 10606\n",
      "Testing set length for 9064: 3535\n",
      "Cross-validation set length for 9064: 3535\n"
     ]
    }
   ],
   "source": [
    "stocks = ['9061', '9062', '9063', '9064', '9065']\n",
    "\n",
    "dfs = {}\n",
    "dfs_cv = {}\n",
    "dfs_test = {}\n",
    "\n",
    "for s in stocks:\n",
    "    df, df_cv, df_test = lob.load_data(s, data_dir='data/INDEX/', cv=True)\n",
    "    dfs[s] = df\n",
    "    dfs_cv[s] = df_cv\n",
    "    dfs_test[s] = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs[stocks[0]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_classification(d, kernel, gamma='auto', C=1.0, degree=3, coef0=0.0, decision_function_shape='ovr'):\n",
    "    clf = svm.SVC(kernel=kernel, gamma=gamma, C=C, degree=degree, coef0=coef0, \n",
    "                  decision_function_shape=decision_function_shape)\n",
    "    X = d['queue_imbalance'].values.reshape(-1, 1)\n",
    "    y = d['mid_price_indicator'].values.reshape(-1, 1)\n",
    "    clf.fit(X, y)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "\n",
    "We will use at first naive approach to grasp how each of the parameter influences the ROC area score and what values make sense, when the other parameters are set to defaults. For the **linear** kernel according to documentation it's worth to check only the **C** parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C parameter\n",
    "\n",
    "The C parameter has influence over margin picked by SVM:\n",
    "* for large values of **C** SVM will choose a smaller-margin hyperplane, which means that more data points will be classified correctly\n",
    "* for small values of **C** SVM will choose a bigger-margin hyperplane, so there may be more misclassifications\n",
    "\n",
    "At first we tried parameters: [0.0001, 0.001, 0.01, 0.1, 1, 10, 1000], but after first calculations it seems that it wasn't enough, so a few more values were introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = [0.0001, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 1.5, 10, 100, 110, 1000]\n",
    "\n",
    "df_css = {}\n",
    "\n",
    "ax = plt.subplot()\n",
    "ax.set_xscale(\"log\", basex=10)\n",
    "for s in stocks:\n",
    "    df_cs = pd.DataFrame(index=cs)\n",
    "    df_cs['roc'] = np.zeros(len(df_cs))\n",
    "    for c in cs:\n",
    "        reg_svm = svm_classification(dfs[s], 'linear', C=c)\n",
    "        pred_svm_out_of_sample = reg_svm.predict(dfs_cv[s]['queue_imbalance'].values.reshape(-1, 1))\n",
    "        logit_roc_auc = roc_auc_score(dfs_cv[s]['mid_price_indicator'], pred_svm_out_of_sample)\n",
    "        df_cs.loc[c] = logit_roc_auc\n",
    "    plt.plot(df_cs, linestyle='--', label=s, marker='x', alpha=0.6)\n",
    "    df_css[s] = df_cs\n",
    "plt.legend()\n",
    "plt.xlabel('C parameter')\n",
    "plt.ylabel('roc_area value')\n",
    "plt.title('roc_area vs C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stocks:\n",
    "    idx = df_css[s]['roc'].idxmax()\n",
    "    print('For {} the best is {}'.format(s, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in stocks:\n",
    "    err_max = df_css[s]['roc'].max()\n",
    "    err_min = df_css[s]['roc'].min()\n",
    "    print('For {} the diff between best and worst {}'.format(s, err_max - err_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "We compare results of SVMs with the best choice of **C** parameter against the logistic regression and SVM with defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(index=stocks)\n",
    "df_results['logistic'] = np.zeros(len(stocks))\n",
    "df_results['linear-default'] = np.zeros(len(stocks))\n",
    "df_results['linear-tunned'] = np.zeros(len(stocks))\n",
    "\n",
    "plt.subplot(121)\n",
    "for s in stocks:\n",
    "    reg_svm = svm_classification(dfs[s], 'linear', C=df_css[s].idxmax())\n",
    "    score = lob.plot_roc(dfs_test[s], reg_svm, title='ROC for test set with the best C param')\n",
    "    df_results['linear-tunned'][s] = score\n",
    "\n",
    "plt.subplot(122)\n",
    "for s in stocks:\n",
    "    reg_svm = svm_classification(dfs[s], 'linear')\n",
    "    score = lob.plot_roc(dfs_test[s], reg_svm, title='ROC for test set with default')\n",
    "    df_results['linear-default'][s] = score\n",
    "\n",
    "plt.subplots_adjust(left=0, wspace=0.1, top=1, right=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "\n",
    "for s in stocks:\n",
    "    reg_svm = svm_classification(dfs[s], 'linear', C=df_css[s].idxmax())\n",
    "    score = lob.plot_roc(dfs_test[s], reg_svm, title='ROC for test set with the best C param')\n",
    "    df_results['linear-tunned'][s] = score\n",
    "\n",
    "plt.subplot(122)\n",
    "for s in stocks:\n",
    "    reg_log = lob.logistic_regression(dfs[s], 0, len(dfs[s]))\n",
    "    \n",
    "    score = lob.plot_roc(dfs_test[s], reg_log, title='ROC for test set with logistic classification')\n",
    "    df_results['logistic'][s] = score\n",
    "\n",
    "plt.subplots_adjust(left=0, wspace=0.1, top=1, right=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
